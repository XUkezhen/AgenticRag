import os
import time
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from rapidocr_onnxruntime import RapidOCR  # å¦‚æœä½ è¦æ”¯æŒå›¾ç‰‡/æ‰«æä»¶

from llm import get_embeddings
from config import CHROMA_DIR


# --- è¾…åŠ©å‡½æ•°ï¼šæå–å›¾ç‰‡æ–‡å­— ---
def extract_text_from_image(img_path):
    try:
        engine = RapidOCR()
        result, _ = engine(img_path)
        if not result: return ""
        return "\n".join([line[1] for line in result])
    except Exception as e:
        print(f"[OCR Error] {e}")
        return ""


async def ingest_file(file_path: str, session_id: str):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ä¸Šä¼ ï¼Œå¸¦è¿›åº¦æ¡å’Œåˆ†æ‰¹å†™å…¥
    """
    print(f"--- [å¼€å§‹å¤„ç†] {os.path.basename(file_path)} (Session: {session_id}) ---")

    # 1. åŠ è½½æ–‡æ¡£
    docs = []
    try:
        if file_path.lower().endswith(".pdf"):
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        elif file_path.lower().endswith(".docx"):
            loader = Docx2txtLoader(file_path)
            docs = loader.load()
        elif file_path.lower().endswith((".jpg", ".jpeg", ".png")):
            text = extract_text_from_image(file_path)
            docs = [Document(page_content=text, metadata={"source": file_path})]
        else:
            print(f"[WARN] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
            return
    except Exception as e:
        print(f"[Load Error] åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        return

    if not docs:
        print("[Warn] æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è§£æå¤±è´¥")
        return

    print(f"-> æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(docs)} é¡µ/éƒ¨åˆ†ï¼Œæ­£åœ¨åˆ‡åˆ†...")

    # 2. åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # ç¨å¾®æ”¹å°ä¸€ç‚¹ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    chunks = splitter.split_documents(docs)

    # 3. æ³¨å…¥ Metadata
    for chunk in chunks:
        chunk.metadata["session_id"] = session_id
        chunk.metadata["source"] = os.path.basename(file_path)

    total_chunks = len(chunks)
    print(f"-> åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {total_chunks} ä¸ªåˆ‡ç‰‡ã€‚å‡†å¤‡å¼€å§‹ Embedding å…¥åº“...")

    # 4. åˆå§‹åŒ–å‘é‡åº“
    embeddings = get_embeddings()
    vectorstore = Chroma(
        persist_directory=CHROMA_DIR,
        embedding_function=embeddings,
        collection_name="chat_docs"
    )

    # 5. ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šåˆ†æ‰¹å†™å…¥ + è¿›åº¦æ‰“å°
    # æ¯æ¬¡å¤„ç† 50 ä¸ªåˆ‡ç‰‡ï¼Œé¿å… API è¶…æ—¶æˆ–æ•°æ®åº“å¡æ­»
    BATCH_SIZE = 50

    for i in range(0, total_chunks, BATCH_SIZE):
        batch = chunks[i: i + BATCH_SIZE]

        try:
            # å†™å…¥ Chroma (è¿™ä¸€æ­¥ä¼šè°ƒç”¨é˜¿é‡Œäº‘ API)
            vectorstore.add_documents(batch)

            # è®¡ç®—è¿›åº¦
            progress = min(i + BATCH_SIZE, total_chunks)
            percent = (progress / total_chunks) * 100
            print(f"   [å†™å…¥ä¸­] {progress}/{total_chunks} ({percent:.1f}%) ...")

            # ğŸ’¡ ç¨å¾®ä¼‘æ¯ä¸€ä¸‹ï¼Œé˜²æ­¢è§¦å‘é˜¿é‡Œäº‘ API çš„ QPS é™åˆ¶ï¼ˆæ¯ç§’è¯·æ±‚è¿‡å¤šä¼šè¢«å°ï¼‰
            time.sleep(0.5)

        except Exception as e:
            print(f"âŒ [Error] æ‰¹æ¬¡ {i} å†™å…¥å¤±è´¥: {e}")
            # å¯ä»¥é€‰æ‹© continue è·³è¿‡é”™è¯¯æ‰¹æ¬¡ï¼Œæˆ–è€… break
            continue

    print(f"âœ… [å®Œæˆ] æ–‡ä»¶ {os.path.basename(file_path)} å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼")